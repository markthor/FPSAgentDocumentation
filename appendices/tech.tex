%!TEX root = ../preamble.tex

\section{Technical Description}

\subsection{Hardware used for training}
\label{sub:hardware}
All training of the CNN was done on a server, at the IT University of Copenhagen, with the following specs:

\begin{description}
	\item [CPU] Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-5820K
	\item [GPU] 2x GeForce\textsuperscript{\textregistered} GTX TITAN X
	\item [RAM] 32 GB HyperX Fury DDR4 2400 C15
\end{description}	

\noindent
Each training experiment was done using one of the GPUs and up to 12 GB of RAM allowing for two simultaneous runs.

\subsection{Communication between the AIC and the VRC}
\label{sec:com-vrc-aic}
The game used throughout the project was made using the game engine Unity, where it is possible to interface with the game using either C\#, JavaScript, Boo or Unity Script. Since the game has to send the pixel data to a CNN to obtain the inputs for the AIC, the easiest option would be to implement the VRC in one of the languages supported by Unity.

Unfortunately we were unable to find a framework suitable for our needs, why we decided to look for frameworks in other languages. Java is the language we are most comfortable in, why we decided to use a Java framework, called DeepLearning4J\footnote{\url{https://deeplearning4j.org/}}.

Neither of the languages used to interface with the game can natively communicate with Java. In order to allow communication between the VRC and the AIC a simple socket bridge was implemented. The overhead introduced by this bridge is minimal as it is possible to send the required data back and forth 10,000 times in approximately three seconds.

\section{Incorrect predictions}
\label{sec:incorrectpredictions}
The following examples are incorrect predictions by the deep visual partitioning convolutional neural network. The green squares mark the ground truths, and the red squares mark the wrong predictions. Absence of either the red or the green square means that either the the prediction  or the ground truth is that no target is present on the screen.

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{0.85\textwidth}
		\input{img/failcollection1.pdf_tex}
	\end{scriptsize}
	\end{center}
\end{figure}


\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{0.85\textwidth}
		\input{img/failcollection2.pdf_tex}
	\end{scriptsize}
	\end{center}
\end{figure}

\section{Angular representation error}
\label{sec:angular-error}
\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{1\textwidth}
		\input{img/angularError.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[AR mean error visualised]{The blue box marks the average error margins for horizontal and vertical angles for the deep CNN without visual distortion, while the red box marks the same error for the shallow CNN.}
	\label{fig:angularerror}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{0.8\textwidth}
		\input{img/aecollection1.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[AR error examples visualised]{The green dot marks the correct position and the red dot marks the estimated position using the deep network.}
	\label{fig:aecollection}
\end{figure}

\section{Visually distorted examples}
\label{sec:vdexamples}
\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/vd1.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Visually distorted example 1]{The deep CNN estimating the VPR correctly predicts the class(id 0) with a confidence of 71.9\%.}
	\label{fig:vd1}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/vd2.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Visually distorted example 2]{The deep CNN estimating the VPR correctly predicts the class(id 4) with a confidence of 78.0\%.}
	\label{fig:vd2}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/vd3.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Visually distorted example 3]{The deep CNN estimating the VPR incorrectly predicts the class(id 2) with a confidence of 58.9\%. The correct class has id 4.}
	\label{fig:vd3}
\end{figure}

\section{Feature maps}
\label{sec:featuremaps-appendix}
The following feature maps are from the image in figure~\ref{fig:hp1}.

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/hardprediction.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Feature maps input image]{The green square marks the correct class.}
	\label{fig:hp1}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/featuremapsvpr2.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Feature maps for the CNN using VPR]{The activations of the convolutional layers in the deep CNN approximating the VPR. The feature maps highlights the position of the target behind the weapon overlay.}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\begin{scriptsize}
		\sffamily
		\input{img/featuremapsar2.pdf_tex}
	\end{scriptsize}
	\end{center}
	\caption[Feature maps for the CNN using the AR]{The activations of the convolutional layers in the deep CNN approximating the AR. The feature maps vaguely highlights the position of the target behind the weapon overlay.}
\end{figure}


\section{Partitioning scheme}

\begin{figure}[H]
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{\textwidth}
		\input{img/grid.pdf_tex}
	\end{scriptsize}
	\caption{Ids of the 3, 3, 3 partitioning scheme}
	\label{fig:ids}
\end{figure}

\section{Neuroevolution graphs}
\label{sec:neuroevolution-graphs}
\input{graph/neat/neat-averaged-aiming-fitness.tex} %DONE
\input{graph/neat/neat-averaged-shooting-fitness.tex} %DONE

\input{graph/neat/neat-averaged-angular-fitness.tex} %DONE
\input{graph/neat/neat-averaged-angular-recoil-fitness.tex} %DONE
\input{graph/neat/neat-vpr-fitness.tex} %DONE
\input{graph/neat/neat-averaged-vpr-recoil-fitness.tex} %DONE

\input{graph/neat/neat-averaged-wrong-reloads.tex}
\input{graph/neat/neat-averaged-missed-shots.tex}

\input{graph/neat/neat-grayscale-fitness.tex}

\section{Deeper convolutional neural networks}
\label{sec:deeper-cnns}
The deeper CNN estimating the AR was trained in the same way as the networks described in section~\ref{sec:approach}, except that the batch size was set to 5 due to memory implications. The deeper network has an additional convolutional layer and max pooling layer as well as 5 fully connected layers with 1,000 neurons in each instead of 3 layers with 250. This sums up to 16 layers. The graphs show that the deeper network has approximately half the horizontal and vertical mean error of the deep network.

\input{graph/score-light-angular-ultradeep.tex}
\input{graph/angular-mse-light-ultradeep.tex}
