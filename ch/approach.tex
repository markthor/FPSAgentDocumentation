%!TEX root = ../preamble.tex

\section{Approach}
\label{sec:approach}
To answer the research question from section~\ref{sub:research-question}, we train a CNN as well as an ANN with neuroevolution, combine them and run varying experiments. This section describes the details of how the networks subject to experimentation are trained and combined.
\todo{read source comment}
%This section needs structure. We need to decide on large subsections. Maybe these 3: Training the CNN; Evaluation for neuroevolution; Combining... feel free to rename

NEAT data gen \textrightarrow{} Data  \textrightarrow{} DB \textrightarrow{} CNN Trainer  \textrightarrow{} CNN Network
\\
NEAT \textrightarrow{} Picture \textrightarrow{} CNN Network \textrightarrow{} Output \textrightarrow{} Agent
\\

\subsection{Training the convolutional neural network}
This section describes the details of the training of the CNN, including the training examples, the network topology and the hyperparameters. 

\subsubsection{Topologies}
We present four different topologies, two for classification and two for regression. Both types are tested with a 12 layered architecture and a 6 layered architecture, called deep and shallow respectively. The difference between the two topologies is in the number of neurons in the fully connected layers and in the activation functions used for the output. All activation functions are ReLU(rectifier), except in the output layers, where softmax is used for classification and identity is used for regression. The regression network outputs the angular representation as described in section \ref{sec:angular}, scaled to fit a range of $-1$ to $1$. The classification network outputs 26 probabilities summing to $1$. The networks take inputs with a shape of 256x256x3, which is the 3 color channels of the image.

\paragraph{Deep neural network topologies}
The network topologies are illustrated on figure \ref{fig:architectureOfDeepNet}. Both have 12 layers, the first 8 being alternating convolutional and pooling layers, while the next 3 layers being fully connected layers followed by an output layer.  The stride and zero padding of all the convolutional layers are 1, while the stride of all the pooling layers is 2. Both networks have a total of 3,200 parameters(weights and biases) in the convolutional layers.
The classification network has 1,228,800 weights($16 \cdot 16 \cdot 120 \cdot 40$) from the last convolutional layer to the first fully connected layer and 4,386 parameters in the remaining layers.
The regression network has 7,680,000 weights($16 \cdot 16 \cdot 120 \cdot 250$) from the last convolutional layer to the first fully connected layer and 126,754 parameters in the remaining layers.

\begin{figure}[H]
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{\textwidth}
		\input{img/architectureOfDeepNet.pdf_tex}
	\end{scriptsize}
	\caption{The full topology of the deep convolutional neural networks.}
	\label{fig:architectureOfDeepNet}
\end{figure}

\paragraph{Shallow neural network topologies}
The network topologies are illustrated on figure \ref{fig:architectureOfShallowNet}. Both have 6 layers, the first 4 being alternating convolutional and pooling layers, followed by a fully connected layer and an output layer.  The stride and zero padding of all the convolutional layers are 1, while the stride of all the pooling layers is 4. Both networks have a total of 5200 parameters(weights and biases) in the convolutional layers.
The classification network has 3,686,400 weights($16 \cdot 16 \cdot 120 \cdot 120$) from the last convolutional layer to the first fully connected layer and 3,266 parameters in the remaining layers.
The regression network has 23,040,000 weights($16 \cdot 16 \cdot 120 \cdot 750$) from the last convolutional layer to the first fully connected layer and 3,754 parameters in the remaining layers.

\begin{figure}[H]
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{\textwidth}
		\input{img/architectureOfShallowNet.pdf_tex}
	\end{scriptsize}
	\caption{The full topology of the relatively shallow convolutional neural networks.}
	\label{fig:architectureOfShallowNet}
\end{figure}

\subsubsection{Training with gradient descent}
The optimisation process of gradient descent did not include continuous evaluation on a validation set for early stopping, as overfitting was of no concern. When the error of the network stopped improving, the process was stopped. The parameters were updated from a mini-batch of size 32, with a learning rate of $10^{-3}$. The process did not include dropout, but included L2-regularization with a coefficient of $5 \cdot 10^{-4}$ for some of the experiments. Nesterov's accelerated gradient was used with a momentum coefficient of $0.9$. The networks were initialised with Xavier initialisation. The number of training examples used for training was 130,000 for all experiments. The framework used for supervised learning was DL4J, a deep learning framework for java, accessable at \url{https://deeplearning4j.org}.

\subsubsection{Training data}
The training examples consists of the raw image data and ground truths. The raw image data is, for each pixel, the byte value of the red, green and blue pixel, i.e. three numbers between 0-255, arranged as a 3 dimensional volume, as seen in figure \ref{fig:split}.

\begin{figure}[H]
    \centering
    \includesvg[svgpath = img/]{RGBtransformation}
    \caption{An image split into its three color channels.}
    \label{fig:split}
\end{figure}
\noindent
The ground truth is different for the visual partitioning representation and the angular representation. For the VPR it is a vector with a 1 in the correct class, and for the angular representation it is a 4 dimensional vector with the correct representation.
In section~\ref{subsub:data-gen} it is explained how the training data for this project was gathered.


\subsubsection{Binarisation of the visual partitioning representation}
The output of the CNN approximating the visual partitioning representation is a probability distribution summing to 1, as described in section \ref{sec:vpr}. When the CNN is used as input provider to the evolved ANN, output of the CNN is transformed to a sparse vector with a 1 in the class with the highest probability. This makes the input similar to the input that the evolved ANN is trained with.

\subsubsection{Scaling of the angular representation}
The angular representation of the position of the target consists of four dimensions as described in section \ref{sec:angular}. The horizontal and vertical angle are scaled to $[-1,1]$, the distance is divided by $20$, and the bit indicating whether the target is within sight is either 1 or 0. When the angular representation is estimated by a CNN and used as input to the evolved ANN, angles are cutoff if they are outside the range of $[-1,1]$, and the indication of whether a target is present is rounded to 0 or 1. If the indication is rounded to 0, the other dimensions are set to 0 for consistency, regardless of the output of the CNN.

%\subsection{Visual partitioning}
%To train the CNN to be able to classify images, classes are needed. Two obvious classes are pictures with and without an enemy. Having only these classes does not provide enough information for NEAT to be able to produce intelligent behaviour. In other words, more classes are needed.
%\todo{Basically already explained in section solutions}
%Introducing more classes is achieved by splitting the image into partitions, where an image belongs to a class if there is an enemy in the corresponding partition.

%Figure~\ref{fig:image-partition}\todo{insert figure} shows an example image that has been split into nine squares. The centre square has been further split into nine squares as well. This partitioning scheme is called a 3, 3 partitioning scheme as the image is split into three squares both horizontally and diagonally, and the same is done to the centre square.

%A 3, 3, 3 partitioning scheme is almost the same as before, except the centre square is further split into three squares both horizontally and diagonally.

%Using a 3, 3, 3 partitioning scheme results in 26 different classes. This is calculated as $(3*3-1)+(3*3-1)+(3*3)+1$. One is subtracted from $(3*3)$ in the first two occurrences, as their centre square is further partitioned. The $+1$ in the end is the class that states there is not an enemy in the picture. The 25 other classes are images where an enemy is in one of the 25 different partitions.

% SKRIV VIDERE

%\input{fig/image-partition.tex}

%Having a fully automated process of producing pixel data and labels allowed different arena setups to be tested quickly.

%The data was saved in a database to make it possible for multiple computers across different networks to both retrieve from and persist to the database.

\subsection{Training the agent}
\label{sub:training-neat}
The agent was trained with a modified version of NEAT as described in section \ref{sec:neat}\todo{describe the sharpNEAT framework}. The training of the agent was done in Unity 5\todo{link?}, using the UnityNEAT framework\todo{https://github.com/lordjesus/UnityNEAT}, which is a, slightly modified, implementation of NEAT and made easy to use with games developed in Unity.
Training the agent using the CNN as the ground truth provider is extremely computationally heavy using the setup of this project and therefore not feasible. Having access to more computational resources or more low level control with the hardware might overcome these challenges. Since neither was available at the time of writing, the CNN was not used as the ground truth provider. Instead of having the CNN detect which visual partition the enemy is in, it is calculated using the state of the game. This method is far less computationally heavy, making the training of the agent a lot faster.

\subsubsection{Evaluation}
The fitness of a network was evaluated in the FPS game described in section \ref{sec:fpssetting}. An evaluation lasts 15 seconds, and as the evaluation includes some randomness, each evaluation was repeated 20 times and the results averaged. The random spawn of the target, and agent, ensures that the agent does not learn a specific pattern, but learns to act in any situation. The agent was awarded for hitting the target and aiming closer to the target.

\subsubsection{Fitness function}
The fitness function for the evaluation can be formulated as:
$$f = \text{damage} + \text{aim\_bonus}$$
Where damage is the total damage dealt, and aim\_bonus is calculated as:
$$ \text{aim\_bonus} = \frac{1}{n} \sum_{n} \frac{c}{(1+v)^2} $$
Where c is a constant set to 75, $n$ is the number of frames and $v$ is the angle measured in radians between the forward pointing vector of the agent and the line from the agent to the target. Hence $v = 0$ if the agent aims directly at the target. This bonus makes the evolution faster, as dealing damage requires aiming, and evolving the ability to aim takes several generations.
\begin{figure}[H]
\centering
\begin{tikzpicture}
	\begin{axis}[
		scale only axis,
		height=5cm,
		width=10cm,
		domain={0:3.1415926535},
		samples=30,
		xmin=0,   xmax=3.1415926535,
		ymin=0,   ymax=75,
		xlabel=$v$,
		ylabel={aim\_bonus}
	]
	% use TeX as calculator:
	\addplot[mark=none]{75 / (1 + x)^2};
	\addplot +[mark=none] coordinates {(0.7333, 0) (0.7333, 75)};
	\end{axis}
\end{tikzpicture}
\caption{The fitness awarded as a function of the angle between the current and the desired pointing direction. The red line marks the value of $v$ when the target is at the corner of the screen}
\end{figure}

\subsubsection{NEAT setup}
The hyperparameters of NEAT are shown in the table below. The activation function used is sigmoid, and biases are modelled by having a constant input, which when connected to, is the equivalent of a bias. The networks start fully connected and allow recurrent connections to appear.
\input{fig/neat-hyperparameters}

\subsubsection{Data generation}
\label{subsub:data-gen}
Training data representative of what the agent will encounter during actual play, will lead to better performing networks as they should not encounter anything they have not been trained for.

Taking pictures of the environment, in which the agent is during actual play, using random positioning and rotation of the camera should produce representative training data. In our case having the camera at a fixed position and random rotation should suffice as our agent is stationary.

An untrained NEAT agent, basing its actions on the angular game state, exhibits more or less random behaviour. During training the behaviour should become less random and more intelligent, which is desirable under normal circumstances, but not when the agent is used to generate training data. Disabling the fitness function, i.e. always giving a fitness of 0, disables learning, thereby persisting the random behaviour.

We were able to almost fully automate the process of generating training data by disabling the fitness function, making it possible to take a screenshot every $n$-th frame, where $n$ was around 5-10,  thereby getting a set of images representative of the states the agent would be in, during actual play.


\subsubsection{Data cleaning}
Problems can occur when representation of classes are imbalanced, as discussed in section~\ref{sub:data-req}.

To overcome these problems some of the images not containing an enemy was deleted. Before deletion approximately half of the images did not contain an enemy. We decided to delete images such that, after deletion around 15\% of the images did not contain an enemy.

The distribution of the rest of the classes depends heavily on the granularity of the partitioning scheme used. Using a 3, 3, 3 partitioning scheme, the most represented class, not counting images without an enemy, constitutes around 11\% of the images. The least represented class contained a mere 0.0862\% of the samples. In other words our representation of classes were majorly imbalanced as seen in figure \ref{fig:enemy-dist}.

Even though we had these large differences in the number of pictures from each class, the convolutional neural network was able to learn what it should, as it can be seen in our results throughout section~\ref{sec:results}, contrary to the claims of~\cite{balanced-classes}.

\input{fig/distribution.tex}

%\subsection{Training image recognition}
%The convolutional neural network used for image recognition was trained using mini batches. A mini batch consists of $b$, where $b$ is dependent on the experiment, instances of training data randomly chosen between the set of training data constituting the training set, of size $t$.
%
%An experiment usually trains for multiple epochs, where an epoch is defined as $t/b$ iterations through the training set. One could call $t/b$ iterations through the training set, a complete traversal, but since every mini batch is composed of randomly selected instances of training data, one epoch could use the same instance multiple times and not use others at all.
















































