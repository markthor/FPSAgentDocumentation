%!TEX root = ../preamble.tex

\section{Approach}
\label{sec:approach}
To answer the research question from section~\ref{sub:research-question}, we develop two VRCs as well as two AICs, combine them and run varying experiments. This section describes the details of how the networks subject to experimentation are trained and combined.

\subsection{Training the convolutional neural network}
This section describes the details of the training of the CNN, including the training examples, the network topology and the hyperparameters. 

\subsubsection{Topologies}
\label{sec:approachtopologies}
We present four different topologies, two for classification and two for regression. Both types are tested with a 12 layered architecture and a 6 layered architecture, referred to as deep and shallow respectively. The difference between the two topologies is in the number of neurons in the fully connected layers and in the activation functions used for the output. All activation functions are ReLU(rectifier), except in the output layers, where softmax is used for classification and identity is used for regression. The regression network outputs the angular representation as described in section \ref{sec:angular}, scaled to fit a range of $-1$ to $1$. The classification network outputs 26 probabilities summing to $1$. The networks take inputs with a shape of 256x256x3, which is the 3 color channels of the image.

\paragraph{Deep neural network topologies}
The network topologies are illustrated on figure \ref{fig:architectureOfDeepNet}. Both have 12 layers, the first 8 being alternating convolutional and pooling layers, while the next 3 layers being fully connected layers followed by an output layer.  The stride and zero padding of all the convolutional layers are 1, while the stride of all the pooling layers is 2. Both networks have a total of 3,200 parameters(weights and biases) in the convolutional layers.
The classification network has 1,228,800 weights($16 \cdot 16 \cdot 120 \cdot 40$) from the last convolutional layer to the first fully connected layer and 4,386 parameters in the remaining layers.
The regression network has 7,680,000 weights($16 \cdot 16 \cdot 120 \cdot 250$) from the last convolutional layer to the first fully connected layer and 126,754 parameters in the remaining layers.

\begin{figure}[H]
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{\textwidth}
		\input{img/architectureOfDeepNet.pdf_tex}
	\end{scriptsize}
	\caption{The full topology of the deep convolutional neural networks.}
	\label{fig:architectureOfDeepNet}
\end{figure}

\paragraph{Shallow neural network topologies}
The network topologies are illustrated on figure \ref{fig:architectureOfShallowNet}. Both have 6 layers, the first 4 being alternating convolutional and pooling layers, followed by a fully connected layer and an output layer.  The stride and zero padding of all the convolutional layers are 1, while the stride of all the pooling layers is 4. Both networks have a total of 5200 parameters(weights and biases) in the convolutional layers.
The classification network has 3,686,400 weights($16 \cdot 16 \cdot 120 \cdot 120$) from the last convolutional layer to the first fully connected layer and 3,266 parameters in the remaining layers.
The regression network has 23,040,000 weights($16 \cdot 16 \cdot 120 \cdot 750$) from the last convolutional layer to the first fully connected layer and 3,754 parameters in the remaining layers.

\begin{figure}[H]
	\begin{scriptsize}
		\sffamily
		\def\svgwidth{\textwidth}
		\input{img/architectureOfShallowNet.pdf_tex}
	\end{scriptsize}
	\caption{The full topology of the relatively shallow convolutional neural networks.}
	\label{fig:architectureOfShallowNet}
\end{figure}

\subsubsection{Training with gradient descent}
The optimisation process of gradient descent did not include continuous evaluation on a validation set for early stopping, as overfitting was of no concern. When the error of the network stopped improving, the process was stopped. The parameters were updated from a mini-batch of size 32, with a learning rate of $10^{-3}$. The process did not include dropout, but included L2-regularization with a coefficient of $5 \cdot 10^{-4}$ for some of the experiments. Nesterov's accelerated gradient was used with a momentum coefficient of $0.9$. The networks were initialised with Xavier initialisation. The number of training examples used for training was 130,000 for all experiments. The framework used for supervised learning was DL4J, a deep learning framework for java, accessable at \url{https://deeplearning4j.org}.

\subsubsection{Training data}
The training examples consists of the raw image data and ground truths. The raw image data is, for each pixel, the byte value of the red, green and blue pixel, i.e. three numbers between 0-255, arranged as a 3 dimensional volume, as seen in figure \ref{fig:split}.

\begin{figure}[H]
    \centering
    \includesvg[svgpath = img/]{RGBtransformation}
    \caption{An image split into its three color channels.}
    \label{fig:split}
\end{figure}
\noindent
The ground truth is different for the two representations. For the VPR it is a vector with a 1 in the correct class, and for the AR it is a 4 dimensional vector with the correct representation.
In section~\ref{subsub:data-gen} it is explained how the training data for this project was gathered.


\subsubsection{Binarisation of the visual partitioning representation}
The output of the VRC approximating the visual partitioning representation is a probability distribution summing to 1, as described in section \ref{sec:vpr}. When the VRC is used as input provider to the AIC, output of the VRC is transformed to a sparse vector with a 1 in the class with the highest probability. This makes the input similar to the input that the AIC is trained with.

\subsubsection{Scaling of the angular representation}
The AR of the position of the target consists of four dimensions as described in section \ref{sec:angular}. The horizontal and vertical angle are scaled to $[-1,1]$, the distance is divided by $20$, and the bit indicating whether the target is within sight is either 1 or 0. When the AR is estimated by the VRC and used as input to the AIC, angles are cutoff if they are outside the range of $[-1,1]$, and the indication of whether a target is present is rounded to 0 or 1. If the indication is rounded to 0, the other dimensions are set to 0 for consistency, regardless of the output of the VRC. When the AR is used as input to the AIC, each angle is split into two scalars. For both angles the scalars are defined as:

\[
    f_1(v) =  
\begin{dcases}
    v,& \text{if } v > 0\\
    0,              & v \leq 0
\end{dcases}
\]

\[
    f_2(v) =  
\begin{dcases}
    -v,& \text{if } v < 0\\
    0,              & v \geq 0
\end{dcases}
\]
\noindent
This results in 6 non-negative real numbers making up the AR.

\subsection{Training the agent}
\label{sub:training-neat}
We obtained the base game and textures used as test bed for our experiments from \url{http://armedunity.com/files/file/107-multiplayer-fps-kit-raknet/} and heavily modified it to suit our needs. This includes removing the ability for bots to shoot and removing everything regarding multiplayer and server functionality. 

The agent was trained with a modified version of NEAT as described in section \ref{sec:neat}\todo{describe the sharpNEAT framework}. The training of the agent was done in Unity 5\todo{link?}, using the UnityNEAT framework\todo{https://github.com/lordjesus/UnityNEAT}, which is a, slightly modified, implementation of NEAT, made easier to use with games developed in Unity.
Training the agent using the CNN as the ground truth provider is extremely computationally heavy using the setup of this project and therefore not feasible. Having access to more computational resources or more low level control with the hardware might overcome these challenges. Since neither was available at the time of writing, the CNN was not used as the ground truth provider. Instead of using the CNN to estimate the representation, it is calculated using the state of the game. This method is far less computationally heavy, making the training of the agent a lot faster.

\subsubsection{Evaluation}
Network fitness was evaluated in the FPS game described in section \ref{sec:fpssetting}. An evaluation lasts 15 seconds, and as the evaluation includes some randomness, each evaluation was repeated 20 times and the results averaged. The random spawn of the target, and agent, ensures that the agent does not learn a specific pattern, but learns to act in any situation. The agent was awarded for hitting the target and aiming closer to the target.

\subsubsection{Fitness function}
The fitness function for the evaluation can be formulated as:
$$f = \text{damage} + \text{aim\_bonus}$$
Where damage is the total damage dealt, and aim\_bonus is calculated as:
$$ \text{aim\_bonus} = \frac{1}{n} \sum_{n} \frac{c}{(1+v)^2} $$
Where c is a constant set to 75, $n$ is the number of frames and $v$ is the angle measured in radians between the forward pointing vector of the agent and the line from the agent to the target. Hence $v = 0$ if the agent aims directly at the target. This bonus makes the evolution faster, as dealing damage requires aiming, and evolving the ability to aim takes several generations.
\begin{figure}[H]
\centering
\begin{tikzpicture}
	\begin{axis}[
		scale only axis,
		height=5cm,
		width=10cm,
		domain={0:3.1415926535},
		samples=30,
		xmin=0,   xmax=3.1415926535,
		ymin=0,   ymax=75,
		xlabel=$v$,
		ylabel={aim\_bonus}
	]
	% use TeX as calculator:
	\addplot[mark=none]{75 / (1 + x)^2};
	\addplot +[mark=none] coordinates {(0.7333, 0) (0.7333, 75)};
	\end{axis}
\end{tikzpicture}
\caption[Fitness function used for neuroevolution]{The fitness awarded as a function of the angle between the current and the desired pointing direction. The red line marks the value of $v$ when the target is at the corner of the screen}
\end{figure}

\subsubsection{NEAT setup}
The hyperparameters of NEAT are shown in table~\ref{tab:neat-hyperparams}. The activation function used is sigmoid, and biases are modelled by having a constant input, which when connected to, is the equivalent of a bias. The networks start fully connected and allow recurrent connections to appear.
\input{fig/neat-hyperparameters}

\subsubsection{Data generation}
\label{subsub:data-gen}
Training data representative of what the agent will encounter during actual play, will lead to better performing networks as they should not encounter anything they have not been trained for.

Taking pictures of the environment, in which the agent is during actual play, using random positioning and rotation of the camera should produce representative training data. In our case having the camera at a fixed position and random rotation should suffice as our agent is stationary.

An untrained NEAT agent, basing its actions on the angular game state, exhibits more or less random behaviour. During training the behaviour should become less random and more intelligent, which is desirable under normal circumstances, but not when the agent is used to generate training data. Disabling the fitness function, i.e. always giving a fitness of 0, disables learning, thereby persisting the random behaviour.

We were able to almost fully automate the process of generating training data by disabling the fitness function and taking a screenshot every 5-10'th frame,  thereby getting a set of images representative of the states the agent would be in, during actual play.


\subsubsection{Data cleaning}
Problems can occur when the representation of classes is imbalanced, as discussed in section~\ref{sub:data-req}.

To overcome these problems some of the images not containing an enemy was deleted. The deletion reduced the number of images with an enemy from \~50\% to \~15\%.

The distribution of the rest of the classes depends heavily on the granularity of the partitioning scheme used. Using a 3, 3, 3 partitioning scheme, the most represented class, not counting images without an enemy, constitutes around 11\% of the images. The least represented class contained a mere 0.0862\% of the samples. In other words our representation of classes were majorly imbalanced as seen in table~\ref{tab:enemy-dist}.

Even though we had these large differences in the number of pictures from each class, the convolutional neural network was able to learn what it should, as it can be seen in our results throughout section~\ref{sec:results}, contrary to the claims of~\cite{balanced-classes}.

\input{fig/distribution.tex}












































