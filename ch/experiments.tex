\section{Experiments}
\label{sec:experiments}

\subsection{Convolutional neural network experiments}
\label{sec:cnnexperiments}
The CNNs described in section \ref{sec:approach} are measured in several ways to find out how well they work individually. Both the visual partitioning and angular representation are measured under different circumstances and with different topologies.

The VPR are measured by the percentage of its prediction that are correct, referred to as accuracy. The class that the CNN assigns the highest probability to is regarded as its prediction. Consequently, whether the CNN predicts with a confidence of $51\%$ or $99\%$ does not change the accuracy, but it changes the cost.

The angular representation is measured in both accuracy percentage and absolute mean error. The horizontal angle, the vertical angle and the distance are measured in absolute mean error while the output indicating whether there is a target present on the image is measured in percentage accuracy. When the angular representation is used as input provider to the evolved ANN, the term indicating whether a target is present is converted to 0 or 1, whichever is closest. Hence the confidence of the prediction does not change its fitness as an input provider. 

To measure whether the CNNs overfits the training data, they are measured against the training set and a test set, using 10,000 samples.

\subsubsection{Visual distortion}
To measure how the network is penalised by having a varying visual representation of the target, the networks are trained with two different data sets, from two different visual settings, as seen in figure \ref{fig:light}. This is relevant, as both the real world and modern FPS games have various visual representations of targets. The textures are more detailed in the visually distorted version and the overlay of the player and the weapon is only present in this setting. The overlay fully or partially covers the target in some cases, making the recognition task harder, or even impossible.

\begin{figure}[H]
	\begin{scriptsize}
		\input{img/withLightWithoutLight.pdf_tex}
	\end{scriptsize}
	\caption{The two different visual settings}
	\label{fig:light}
\end{figure}

\subsubsection{Different topologies}
The effect of depth of the CNN topologies described in section \ref{sec:approachtopologies} are measured in combination with the different visual game settings to assert its effect on training speed, overfitting and accuracy.

\subsubsection{Smaller training sets}
In a real world application of visual recognition, the number of training examples is often limited, as the training examples are subject to manual labeling. Hence, we measure both the accuracy of the VPR and the angular representation trained on significantly smaller data sets from the visually distorted game setting. Furthermore, overfitting is measured separately on these networks trained with smaller data sets, as overfitting gets more likely with fewer training examples. The data sets used are subsets of the training sets with 130,000 examples, and has an even distribution of VPR classes. The training process uses L2-regularisation with $\lambda = 0.0005$.

 
\subsection{Neuroevolution experiments}
The fitness of the ANNs evolved with neuroevolution are measured with the game state as ground truth provider, to assert how well the networks work without being penalised by the error of the VRC. Fitness from aiming and damage dealing are visualised separately to show how the aim fitness helps accelerate learning. We compare the ANNs based on VPR and AR on how fast they learn and how well they perform. Each AIC is evolved 3 times and its fitness averaged, as the evolution process is random.

\subsubsection{Unnecessary reloads and misses}
To find out how well neuroevolution solves the task of reloading and shooting without missing, we measure the number of unnecessary reloads and misses. An unnecessary reload is defined as a reload that does not increase the number of bullets in the magazine.

\subsection{Pipeline experiments}
We evaluate the fitness of the combination of the VRC and the AIC and compare it to the fitness of the same AIC with the ground truths as feature representation. To find out how much the pipeline fitness is penalised from using AICs trained with different providers, we evaluate the fitness of the combination with half the look sensitivity\footnote{The factor that the output of the ANN trained with neuroevolution is multiplied with to calculate the resulting horizontal or vertical rotation speed.}. Lowering look sensitivity might reduce the impact of the VRC error, as the perception is translated more frequently per degree rotated. 






